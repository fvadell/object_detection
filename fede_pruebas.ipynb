{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from model import SSD300, MultiBoxLoss\n",
    "from datasets import PascalVOCDataset\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SSD300(n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frizar: conv 4_3, conv 7, conv 8_2, conv 9_2, conv 10_2, conv 11_2 + pred convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD300(\n",
       "  (base): VGGBase(\n",
       "    (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool5): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux_convs): AuxiliaryConvolutions(\n",
       "    (conv8_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv8_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv9_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv9_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv10_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv10_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv11_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv11_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (pred_convs): PredictionConvolutions(\n",
       "    (loc_conv4_3): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv7): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv8_2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv9_2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv10_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv11_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv4_3): Conv2d(512, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv7): Conv2d(1024, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv8_2): Conv2d(512, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv9_2): Conv2d(256, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv10_2): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv11_2): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model.children():\n",
    "    for param in child.parameters():\n",
    "        #param.requires_grad = False\n",
    "        print(param.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model.children():\n",
    "    for layer in child.children():\n",
    "        for i in layer.parameters():\n",
    "            i.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  conv 4_3, conv 7, conv 8_2, conv 9_2, conv 10_2, conv 11_2 + pred convs\n",
    "layers = [model.base.conv4_3, model.base.conv7, model.aux_convs.conv8_2, model.aux_convs.conv9_2, model.aux_convs.conv10_2, model.aux_convs.conv11_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in layers:\n",
    "    for i in l.parameters():\n",
    "        i.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionConvolutions(\n",
       "  (loc_conv4_3): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (loc_conv7): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (loc_conv8_2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (loc_conv9_2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (loc_conv10_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (loc_conv11_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cl_conv4_3): Conv2d(512, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cl_conv7): Conv2d(1024, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cl_conv8_2): Conv2d(512, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cl_conv9_2): Conv2d(256, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cl_conv10_2): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cl_conv11_2): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pred_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.pred_convs.children():\n",
    "    for j in i.parameters():\n",
    "        j.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGBase(\n",
      "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool5): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "  (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "AuxiliaryConvolutions(\n",
      "  (conv8_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv8_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv9_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv9_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv10_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv10_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv11_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv11_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "PredictionConvolutions(\n",
      "  (loc_conv4_3): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (loc_conv7): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (loc_conv8_2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (loc_conv9_2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (loc_conv10_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (loc_conv11_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cl_conv4_3): Conv2d(512, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cl_conv7): Conv2d(1024, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cl_conv8_2): Conv2d(512, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cl_conv9_2): Conv2d(256, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cl_conv10_2): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cl_conv11_2): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for l in model:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f897359b120>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescale_factors\n",
      "base.conv1_1.weight\n",
      "torch.Size([64, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(param_name)\n",
    "    if param_name == 'base.conv1_1.weight': \n",
    "        print(param.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = [1,2,3]\n",
    "not_biases = [4,5,6]\n",
    "lr = .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [{'params': biases, 'lr': 2 * lr}, {'params': not_biases}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from utils import create_data_lists\r\n",
      "\r\n",
      "#voc07_path = 'data/VOC2007'\r\n",
      "voc07_path = 'data/FedeSet'\r\n",
      "#voc12_path = 'data/VOC2012'\r\n",
      "voc12_path = 'data/Damiset'\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    create_data_lists(voc07_path= voc07_path,\r\n",
      "                      voc12_path= voc12_path,\r\n",
      "                      output_folder='./')\r\n"
     ]
    }
   ],
   "source": [
    "!cat create_data_lists.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arial.ttf\t\t input\t\t\tTRAIN_objects.json\r\n",
      "checkpoints\t\t label_map (fede).json\ttrain.py\r\n",
      "create_data_lists.py\t label_map.json\t\tUntitled.ipynb\r\n",
      "data\t\t\t LICENSE\t\tutils.py\r\n",
      "datasets.py\t\t model.py\t\tutils.pyc\r\n",
      "detect_image.py\t\t outputs\t\tVOCdevkit\r\n",
      "detect.py\t\t __pycache__\t\tVOCdevkit (2)\r\n",
      "detect_vid.py\t\t QueHacer.odt\t\tVOCdevkit (3)\r\n",
      "eval.py\t\t\t README.md\t\tVOCtest_06-Nov-2007.tar\r\n",
      "fede_detect_image.ipynb  TEST_images.json\tVOCtrainval_06-Nov-2007.tar\r\n",
      "fede_pruebas.ipynb\t TEST_objects.json\tVOCtrainval_11-May-2012.tar\r\n",
      "img\t\t\t TRAIN_images.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import PascalVOCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que correr create_data_lists.py antes por afuera. Debería crear un jsons, por ejemplo: TRAIN_images.json\n",
    "# en definitva no es otra cosa más que una lista de paths de imágenes\n",
    "import json\n",
    "\n",
    "f = open('label_map.json',)\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 21)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aeroplane': 1,\n",
       " 'bicycle': 2,\n",
       " 'bird': 3,\n",
       " 'boat': 4,\n",
       " 'bottle': 5,\n",
       " 'bus': 6,\n",
       " 'car': 7,\n",
       " 'cat': 8,\n",
       " 'chair': 9,\n",
       " 'cow': 10,\n",
       " 'diningtable': 11,\n",
       " 'dog': 12,\n",
       " 'horse': 13,\n",
       " 'motorbike': 14,\n",
       " 'person': 15,\n",
       " 'pottedplant': 16,\n",
       " 'sheep': 17,\n",
       " 'sofa': 18,\n",
       " 'train': 19,\n",
       " 'tvmonitor': 20,\n",
       " 'background': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es una parte de train.py, pero antes hay que correr create_data_lists.py\n",
    "# Además a create_data_lists.py le tuve que cambiar el path del dataset\n",
    "\n",
    "# Custom dataloaders\n",
    "keep_difficult = True\n",
    "data_folder = './'\n",
    "\n",
    "# Este constructor de dataset es la posta.\n",
    "# Toma un json con los paths de las imagenes y un json con los boxes, clasificaciones y difcultades\n",
    "# Y arma el objeto dataset con esto\n",
    "\n",
    "train_dataset = PascalVOCDataset(data_folder,\n",
    "                                 split='train',\n",
    "                                 keep_difficult=keep_difficult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],\n",
       "          [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],\n",
       "          [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],\n",
       "          ...,\n",
       "          [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],\n",
       "          [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116],\n",
       "          [-0.0116, -0.0116, -0.0116,  ..., -0.0116, -0.0116, -0.0116]],\n",
       " \n",
       "         [[-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],\n",
       "          [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],\n",
       "          [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],\n",
       "          ...,\n",
       "          [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],\n",
       "          [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049],\n",
       "          [-0.0049, -0.0049, -0.0049,  ..., -0.0049, -0.0049, -0.0049]],\n",
       " \n",
       "         [[-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],\n",
       "          [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],\n",
       "          [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],\n",
       "          ...,\n",
       "          [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],\n",
       "          [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092],\n",
       "          [-0.0092, -0.0092, -0.0092,  ..., -0.0092, -0.0092, -0.0092]]]),\n",
       " tensor([[0.1902, 0.0000, 0.2276, 0.1310],\n",
       "         [0.2337, 0.0530, 0.2877, 0.1653],\n",
       "         [0.3479, 0.0322, 0.3859, 0.1674],\n",
       "         [0.2080, 0.0000, 0.2411, 0.0894]]),\n",
       " tensor([9, 9, 9, 9]),\n",
       " tensor([0, 0, 1, 0], dtype=torch.uint8))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset ahora tiene tuplas de la pinta:\n",
    "train_dataset[0]\n",
    "# que representan la imagen, las cajas, y las categorías (la cuarta creo que es un tag de imagen dificil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "workers = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                           pin_memory=True)  # note that we're passing the collate function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch, channels, dim1, dim2]\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxes tiene los ground truths\n",
    "print(len(boxes))\n",
    "boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels tiene la clasificación verdadera de cada box\n",
    "print(len(labels))\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola qué tal??'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '   Hola Qué Tal??   '\n",
    "string.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd",
   "language": "python",
   "name": "ssd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
